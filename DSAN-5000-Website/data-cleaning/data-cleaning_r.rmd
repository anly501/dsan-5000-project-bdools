---
title: "Data Cleaning in R"
author: "Ben Doolan"
---

# **Analytical Dataframe Cleaning**

Here, I am planning to make use of tidyverse and naniar to clean up the data and visualize the nulls. I'll start by loading in the data and the libraries I'll need. First, I'm going to replace all the blank string cells, "", with NA/NaN values. Then, I'll use the `naniar` library to visualize the nulls in the dataset.


```{r}
library(dplyr)
library(tidyverse)
library(naniar)

analytical_df <- read_csv("./outputs/analytical_df_python_to_r.csv")

print(head(glimpse(analytical_df)))

analytical_df <- analytical_df %>%
  mutate(across(where(is.character), 
                ~ifelse(. == "", NA_character_, .)))

# Print the modified structure of the dataframe
print(head(glimpse(analytical_df)))
```

## **Summarzing nulls removal**

Now, let's see what the data cleaning has done and analyze the distribution of nulls in the dataframe

```{r}

library(dplyr)
library(tidyverse)

summary_df <- analytical_df %>%
  summarise(across(everything(), list(nulls = ~sum(is.na(.)), non_nulls = ~sum(!is.na(.)))))

print(head(glimpse(summary_df)))

gg_miss_var(analytical_df)

gg_miss_upset(analytical_df)

vis_miss(analytical_df)

print(n =23,miss_var_summary(analytical_df))

```

Great, it looks like the majority of our nulls come from the SEC Financial data columns, which is to be expected. Since I want to to have pretty complete quantitative data to perform analysis in the future, I want to find a subset of the data with quantitative nulls minimized. It seems like the most overlap exists between the Gross Profit and SG&A columns, so let's go ahead and look at a subset where any rows with nulls in either of these fields are removed.

```{r}

library(dplyr)
library(tidyverse)

analytical_df <- analytical_df %>%
  filter(!is.na(GrossProfit) & !is.na(SellingGeneralAndAdministrativeExpense)) %>% 
  select(-expedited_review_flag)


vis_miss(analytical_df)

```

As you can see, once I've eliminated the rows with nulls in Gross Profit and SG&A, I'm left with 249 rows of data. This is a good number to work with, so I'll finish up my initial data clean there and pass the variable back into python.

```{r}

library(dplyr)
library(tidyverse)

write_csv(analytical_df, "./outputs/analytical_df_r_to_python.csv")

```

#  **R edgar package API**

Here is a link to the [edgar package documentation](https://cran.r-project.org/web/packages/edgar/vignettes/edgar.html). I will use this package to pull the company description summary from the SEC annual reports.

Specifically, I will be using the getBusinDescr function to pull the company description summary from the annual reports as a text file. I will then save the text files to a folder called `companydescriptions`.

In order to do this I need a vector of the filing range of filing years I am interested in. I will use the `financial_year` column from the analytical_df to create this vector.

```{r}

library(dplyr)
library(tidyverse)
library(edgar)

setwd("./outputs/SEC-Edgar")  ## sets outputs to outputs folder

unique_CIKs <- analytical_df %>%
  distinct(CIK) %>%
  pull()

filing_year_list <- analytical_df %>%
  distinct(financial_year) %>%
  pull()


getBusinDescr(cik.no= unique_CIKs, filing.year = filing_year_list, useragent = "bed35@georgetown.edu")

```